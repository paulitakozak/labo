B = c(1,7,1)
C = A*B
B
C
matriz<-matrix(A,B)
matriz
matriz<-matrix(c(A,B),nrow=3,byrow=F)
matriz
eigen(matriz)
matriz<-matrix(c(A,B,C),nrow=3,byrow=F)
eigen(matriz)
cov<-cov(matriz)
eigen(cov)
corr<-cor(matriz)
corr
eigen(corr)
H<- c(24,6)
M<- c(56,14)
P<-rbind(H,M)
colnames(P)<-(c("E","D"))
P #tabla de contingencia, 2 vbles indep
mosaicplot(P)
# La % de empleados y desempleados para H y M es =
H2<- c(24,60)
M2<- c(56,14)
P2<-rbind(H2,M2)
colnames(P2)<-(c("E","D"))
P2 #tabla de contingencia, 2 vbles dep
mosaicplot(P2)
library(readxl) ##Permite leer archivos xlsx
library(ggplot2) ##Paquete para confeccionar dibujos
library(ggrepel) ##Paquete que manipula etiquetas para gráficos
library(plotrix) ##Paquete para gráficos requerido para la libreria smacof
library(smacof)  ##Paquete para MDS basado en la minimización del stress
library(UsingR)
library(pgirmess)#pruebas a posteriori de kruskal wallis
library(MASS)
library(car)
S1=c(3.3,4.4,4.9,4.9,3.9,4.2,4.7,5.1,4.6,4.5)
S2=c(4.6,4.5,5.0,4.0,4.5,5.2,4.9,5.5,4.8,5.3)
S3=c(6.7,5.8,5.0,4.8,5.3,6.2,5.0,6.4,5.9,5.4)
S4=c(6.3,6.0,6.7,5.5,6.6,6.1,5.3,6.5,6.3,6.8)
supl=cbind(S1,S2,S3,S4)
efic=data.frame("Suplemento"=factor(c(rep(1,10),rep(2,10),rep(3,10),rep(4,10))),
"Eficiencia"=c(S1,S2,S3,S4))
efic
install.packages("ggrepel")
install.packages("plotrix")
install.packages("smacof")
install.packages("smacof")
install.packages("UsingR")
medias<-apply(supl,2,mean)
medias
desvios<-apply(supl,2,sd)
desvios
install.packages("pgirmess")
install.packages("car")
GrupoA=c(25,36,36,25,36,16,25,36,49,36,25)
GrupoB=c(121,36,36,64,36,81,49,25,64,49,121)
GrupoC=c(81,81,36,9,25,36,9,49,169,1,81)
GrupoD=c(25,25,36,25,36,25,25,25,25,25,25)
Tiempos=cbind(GrupoA,GrupoB,GrupoC,GrupoD)
tiempo=data.frame("Grupos"=factor(c(rep(1,11),rep(2,11),rep(3,11),rep(4,11))),
"Tiempos"=c(GrupoA,GrupoB,GrupoC,GrupoD))
tiempo
plot5 <-ggplot(tiempo,aes(x=Grupos, y=Tiempos, fill=Grupos))+ geom_boxplot()
plot5
medias5<-apply(tiempo,2,mean)
medias5
desvios5<-apply(tiempo,2,sd)
desvios5
medias5<-apply(Tiempos,2,mean)
medias5
desvios5<-apply(Tiempos,2,sd)
desvios5
salida5<-round(cbind(media5,desvio5),3)
salida5
salida5<-round(cbind(medias5,desvios5),3)
salida5
Tiempo.anova<-aov(Tiempo-Grupos,data=Tiempos)
Tiempo.anova<-aov(Tiempo-Grupos,data=tiempo)
Tiempo.anova<-aov(Tiempos~Grupos,data=tiempo)
Tiempo.anova
shapiro.test(residuals(Tiempo.anova))
leveneTest(tiempo$Tiempos,tiempo$Grupos)
library(car)
kruskal.test(Tiempos~Grupos,data=tiempo)
library(readxl) ##Permite leer archivos xlsx
library(ggplot2) ##Paquete para confeccionar dibujos
library(ggrepel) ##Paquete que manipula etiquetas para gráficos
library(plotrix) ##Paquete para gráficos requerido para la libreria smacof
library(smacof)  ##Paquete para MDS basado en la minimización del stress
library(UsingR)
library(pgirmess)#pruebas a posteriori de kruskal wallis
library(MASS)
library(car)
tpos1 = c(0.17,0.26,0.19,0.34,0.52,0.33,0.23,0.20,0.18,0.22,
0.21,0.22,0.28,0.25,0.90,0.33,0.22,0.17,0.39,0.27)
tpos2 = c(0.18,0.33,0.23,0.16,0.19,0.30,0.21,0.20,0.16,0.21,
0.20,0.30,0.32,0.20,0.19,0.22,0.27,0.24,0.29,0.27)
tiempo=data.frame(tpos1,tpos2)
#Analizo Normalidad
shapiro.test((tpos1))
shapiro.test(tpos2)
boxcox((tpos1~1,plotit=T))
boxcox(tpos1~1,plotit=T)
t1=tpos1^(-1.5)
t2=tpos2^(-1.5)
boxcox(tiempo~1,plotit=T)
shapiro.test((t1))
shapiro.test(t2)
boxcox(tpos1~1,plotit=T)
boxcox(tpos1~1,plotit=T)
#supongo igualdad de vzas, aplico test de igualdad de medias
t.test(t1,t2,alternative ="two.sided")
library(readxl) ##Permite leer archivos xlsx
library(ggplot2) ##Paquete para confeccionar dibujos
library(ggrepel) ##Paquete que manipula etiquetas para gráficos
library(plotrix) ##Paquete para gráficos requerido para la libreria smacof
library(smacof)  ##Paquete para MDS basado en la minimización del stress
library(UsingR)
library(pgirmess)#pruebas a posteriori de kruskal wallis
library(MASS)
library(car)
tpos1 = c(0.17,0.26,0.19,0.34,0.52,0.33,0.23,0.20,0.18,0.22,
0.21,0.22,0.28,0.25,0.90,0.33,0.22,0.17,0.39,0.27)
tpos2 = c(0.18,0.33,0.23,0.16,0.19,0.30,0.21,0.20,0.16,0.21,
0.20,0.30,0.32,0.20,0.19,0.22,0.27,0.24,0.29,0.27)
tiempo=data.frame(tpos1,tpos2)
shapiro.test((tpos1))
shapiro.test(tpos2)
Arbequina=c(34.5,20.1,21.8,18.2,19.5,20.2,22.5,23.9,22.1,24.2)
Carolea=c(16.4,14.8,17.8,12.3,11.9,15.5,13.4,16,15.8,16.2)
shapiro.test(Arbequina) #Testea normalidad de los datos
# P-valor= 0.00596 < 0.05 -> no normal
shapiro.test(Carolea)
Arbequina=c(34.5,20.1,21.8,18.2,19.5,20.2,22.5,23.9,22.1,24.2)
Carolea=c(16.4,14.8,17.8,12.3,11.9,15.5,13.4,16,15.8,16.2)
shapiro.test(Arbequina) #Testea normalidad de los datos
# P-valor= 0.00596 < 0.05 -> no normal
shapiro.test(Carolea)
wilcox.test(Arbequina,Carolea,alternative ="two.sided")
library(factoextra) # para m�todo de Pearson y heatmaps
library(igraph)
library(tidygraph)
library(ggraph)
library(dplyr) # para manejar bases
library(tidyverse)
library(cluster)
library(mclust)  #ejercicio 3
library("clValid") #validez de los algoritmos
data(USArrests)
# Se escalan las variables
datos <- scale(USArrests, center = TRUE, scale = TRUE)
# Distancia eucl�dea
mat_dist <- dist(x = datos, method = "euclidean")
round(as.matrix(mat_dist)[1:5, 1:5], 2)
mat_dist <- get_dist(x = datos, method = "pearson")
install.packages("factoextra")
#instalamos librer�as
library(factoextra) # para m�todo de Pearson y heatmaps
mat_dist <- get_dist(x = datos, method = "pearson")
round(as.matrix(mat_dist)[1:5, 1:5], 2)
round(as.matrix(mat_dist)[1:5, 1:5], 2)
round(as.matrix(mat_dist)[1:5, 1:5], 2)
fviz_dist(dist.obj = mat_dist, lab_size = 5) +
theme(legend.position = "none")
#la funci�n fviz_nbclust permite analizar la cantidad de clusters
fviz_nbclust(x = datos, FUNcluster = kmeans, method = "wss", k.max = 15,
diss = get_dist(datos, method = "euclidean"), nstart = 50)
set.seed(123)
km_clusters <- kmeans(x = datos, centers = 4, nstart = 50)
# Las funciones del paquete factoextra emplean el nombre de las filas del
# dataframe que contiene los datos como identificador de las observaciones.
# Esto permite a�adir labels a los gr�ficos.
fviz_cluster(object = km_clusters, data = datos, show.clust.cent = TRUE,
ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
labs(title = "Resultados clustering K-means") +
theme_bw() +
theme(legend.position = "none")
km_clusters <- kmeans(x = datos, centers = 4, nstart = 50)
resultados <- data.frame(ciudad  = names(km_clusters$cluster),
cluster = as.factor(km_clusters$cluster)) %>%arrange(cluster)
resultados
medoids <- prcomp(datos)$x
pam_clusters <- pam(x = datos, k = 4, metric = "manhattan")
pam_clusters
fuzzy_cluster <- fanny(x = datos, diss = FALSE, k = 3, metric = "euclidean",
stand = FALSE)
head(fuzzy_cluster$membership)
fviz_cluster(object = fuzzy_cluster, repel = TRUE, ellipse.type = "norm",
pallete = "jco") + theme_bw() + labs(title = "Fuzzy Cluster plot")
data("diabetes")
head(diabetes) # visualizamos las primeras filas
install.packages("mclust")
data("diabetes")
head(diabetes) # visualizamos las primeras filas
library(factoextra) # para m�todo de Pearson y heatmaps
library(igraph)
library(tidygraph)
library(ggraph)
library(dplyr) # para manejar bases
library(tidyverse)
library(cluster)
library(mclust)
data("diabetes")
head(diabetes) # visualizamos las primeras filas
# Estandarizaci�n de variables
datos <- scale(diabetes[, -1])
head(datos)
# clustering basado en modelos
model_clustering <- Mclust(data = datos, G = 1:10)
summary(model_clustering)
# Grado de asignaci�n a cada cluster
head(model_clustering$z)
# Clasificaci�n final
head(model_clustering$classification)
# TP NÂ°4
library(readxl) ##Permite leer archivos xlsx
library(ggplot2) ##Paquete para confeccionar dibujos
library(ggrepel) ##Paquete que manipula etiquetas para grÃ¡ficos
library(plotrix) ##Paquete para grÃ¡ficos requerido para la libreria smacof
library(smacof)  ##Paquete para MDS basado en la minimizaciÃ³n del stress
library(UsingR)
library(pgirmess)#pruebas a posteriori de kruskal wallis
library(MASS)
library(car)
# TP NÂ°4
library(readxl) ##Permite leer archivos xlsx
library(ggplot2) ##Paquete para confeccionar dibujos
library(ggrepel) ##Paquete que manipula etiquetas para grÃ¡ficos
library(plotrix) ##Paquete para grÃ¡ficos requerido para la libreria smacof
library(smacof)  ##Paquete para MDS basado en la minimizaciÃ³n del stress
library(UsingR)
library(pgirmess)#pruebas a posteriori de kruskal wallis
library(MASS)
library(car)
m1<- c(24.4,22.6,23.8,22,24.5,22.3,25,24.5)
m2<- c(10.2,12.1,10.3,10.2,9.9,11.2,12,9.5)
m3<- c(19.2,19.4,19.8,19,19.6,18.3,20,19.4)
m4<- c(17.4,18.1,16.7,18.3,17.6,17.5,18,16.4)
m5<- c(13.4,15,14.1,13.1,14.9,15,13.4,14.8)
marca<- cbind(m1,m2,m3,m4,m5)
marca
sodio<- data.frame("marca"=factor(c(rep(1,8),rep(2,8),rep(3,8),
rep(4,8),rep(5,8),rep(6,8))),
"sodio"=c(marca1,marca2,marca3,marca4,marca5,marac6))
sodio<- data.frame("marca"=factor(c(rep(1,8),rep(2,8),rep(3,8),
rep(4,8),rep(5,8),rep(6,8))),
"sodio"=c(m1,m2,m3,m4,m5,m6))
m6<- c(21.3,20.2,20.7,20.8,20.1,18.8,21.1,20.3)
marca<- cbind(m1,m2,m3,m4,m5,m6)
sodio<- data.frame("marca"=factor(c(rep(1,8),rep(2,8),rep(3,8),
rep(4,8),rep(5,8),rep(6,8))),
"sodio"=c(m1,m2,m3,m4,m5,m6))
sodio
plot2=ggplot(sodio, aes(x=marca, y=sodio,fill= marca)+geom_boxplot())
plot2=ggplot(sodio, aes(marca,sodio,fill= marca)+geom_boxplot())
plot2=ggplot(sodio,aes(marca,sodio,fill=marca)+geom_boxplot())
plot2=ggplot(sodio,aes(x=marca,y=sodio,fill=marca)+geom_boxplot())
plot2=ggplot(sodio,aes(x=marca,y=sodio,fill=marca))+geom_boxplot()
plot2
# Calculo de las medias y desvios estandar de c/u de los grupos
medias<- apply(marca,2,mean)
medias
desvios<- apply(marca,2,sd)
desvios
salida2<- round(cbind(medias,desvios),3)
salida2
sodio.aov<-aov(sodio~marca,data=sodio)
sodio.aov
summary(sodio.aov)
shapiro.test(residuals(sodio.aov))
levene.test(sodio$sodio, marca$marca)
leveneTest(sodio$sodio, marca$marca)
leveneTest(sodio$sodio, sodio$marca)
pairswise.t.test(sodio$sodio, sodio$marca, p.adjust.methods="bonferroni")
library(stats)
pairswise.t.test(sodio$sodio, sodio$marca, p.adjust.methods="bonferroni")
pairwise.t.test(sodio$sodio, sodio$marca, p.adjust.methods="bonferroni")
TukeyHSD(sodio.aov,"marca")
kruskal.test(sodio~marca,data=sodio)
kruskalmc(sodio~marca,data=sodio)
plot(sodio.aov,data=sodio)
library(readxl) ##Permite leer archivos xlsx
library(ggplot2) ##Paquete para confeccionar dibujos
library(ggrepel) ##Paquete que manipula etiquetas para grÃ¡ficos
library(plotrix) ##Paquete para grÃ¡ficos requerido para la libreria smacof
library(smacof)  ##Paquete para MDS basado en la minimizaciÃ³n del stress
library(UsingR)
library(pgirmess)#pruebas a posteriori de kruskal wallis
library(MASS)
library(car)
library(stats)
S1=c(3.3,4.4,4.9,4.9,3.9,4.2,4.7,5.1,4.6,4.5)
S2=c(4.6,4.5,5.0,4.0,4.5,5.2,4.9,5.5,4.8,5.3)
S3=c(6.7,5.8,5.0,4.8,5.3,6.2,5.0,6.4,5.9,5.4)
S4=c(6.3,6.0,6.7,5.5,6.6,6.1,5.3,6.5,6.3,6.8)
supl=cbind(S1,S2,S3,S4)
supl
efic=data.frame("Suplemento"=factor(c(rep(1,10),rep(2,10),rep(3,10),rep(4,10))),
"Eficiencia"=c(S1,S2,S3,S4))
efic
medias<-apply(supl,2,mean)
medias
desvios<-apply(supl,2,sd)
desvios
Resumen3<-round(cbind(medias,desvios),3)
Resumen3
plot3<- ggplot(efic, aes(x=Suplemento, y=Eficiencia, fill=Suplemento)+
geom_boxplot())
plot3
plot3<- ggplot(efic,aes(x=Suplemento, y=Eficiencia, fill=Suplemento))+
geom_boxplot()
plot3
GrupoA=c(25,36,36,25,36,16,25,36,49,36,25)
efic.aov<- aov(Eficiencia~Suplemento, data=efic)
efic.aov
summary(efic.aov)
shapiro.test(residuals(efic.aov))
# 2) Homocedasticidad
leveneTest(efic$Eficiencia,efic$Suplemento)
TukeyHSD(efic.aov,"supl")
TukeyHSD(efic.aov,"Suplemento")
install.packages("UsingR")
GrupoA=c(25,36,36,25,36,16,25,36,49,36,25)
GrupoB=c(121,36,36,64,36,81,49,25,64,49,121)
GrupoC=c(81,81,36,9,25,36,9,49,169,1,81)
GrupoD=c(25,25,36,25,36,25,25,25,25,25,25)
Tiempos=cbind(GrupoA,GrupoB,GrupoC,GrupoD)
tiempo=data.frame("Grupos"=factor(c(rep(1,11),rep(2,11),rep(3,11),rep(4,11))),
"Tiempos"=c(GrupoA,GrupoB,GrupoC,GrupoD))
tiempo
plot5 <-ggplot(tiempo,aes(x=Grupos, y=Tiempos, fill=Grupos))+ geom_boxplot()
plot5
medias5<-apply(Tiempos,2,mean)
medias5
desvios5<-apply(Tiempos,2,sd)
desvios5
salida5<-round(cbind(medias5,desvios5),3)
salida5
Tiempo.anova<-aov(Tiempos~Grupos,data=tiempo)
summaryTiempo.aov)
Tiempo.anova<-aov(Tiempos~Grupos,data=tiempo)
summary(Tiempo.aov)
summary(Tiempo.anova)
library(factoextra) # para mï¿½todo de Pearson y heatmaps
library(igraph)
library(tidygraph)
library(ggraph)
library(dplyr) # para manejar bases
library(tidyverse)
library(cluster)
library(mclust)  #ejercicio 3
library("clValid") #validez de los algoritmos
data(USArrests)
View(USArrests)
dim(USArrests)
# Se escalan las variables
datos <- scale(USArrests, center = TRUE, scale = TRUE)
# Distancia euclidea
mat_dist <- dist(x = datos, method = "euclidean")
round(as.matrix(mat_dist)[1:5, 1:5], 2)
fviz_dist(dist.obj = mat_dist, lab_size = 5) +
theme(legend.position = "top")
#la funcion fviz_nbclust permite analizar la cantidad de clusters
fviz_nbclust(x = datos, FUNcluster = kmeans, method = "wss", k.max = 15,
diss = get_dist(datos, method = "euclidean"), nstart = 50)
set.seed(123)
km_clusters <- kmeans(x = datos, centers = 4, nstart = 50)
resultados <- data.frame(ciudad  = names(km_clusters$cluster),
cluster = as.factor(km_clusters$cluster)) %>%arrange(cluster)
resultados
fviz_cluster(object = km_clusters, data = datos, show.clust.cent = TRUE,
ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
labs(title = "Resultados clustering K-means") +
theme_bw() +
theme(legend.position = "none")
#Usando las componentes principales y distancia Manhatan realizamos la
#clusterizacion y graficamos nuevamente
head(datos)
medoids <- prcomp(datos)$x
medoids
# Se emplean los mismos nombres que en el objeto ggplot
colnames(medoids) <- c("x", "y")
medoids <- as.data.frame(medoids)
# Se emplean los mismos nombres que en el objeto ggplot
colnames(medoids) <- c("x", "y")
# Creacion del grafico
fviz_cluster(object = pam_clusters, data = datos, ellipse.type = "t",
repel = TRUE) +
theme_bw() +
# Se resaltan las observaciones que actï¿½an como medoids
geom_point(data = medoids, color = "firebrick", size = 2) +
labs(title = "Resultados clustering PAM") +
theme(legend.position = "none")
hc_euclidea_complete <- hclust(d = mat_dist, method = "complete")
cor(x = mat_dist, cophenetic(hc_euclidea_complete))
hc_euclidea_average  <- hclust(d = mat_dist, method = "average")
cor(x = mat_dist, cophenetic(hc_euclidea_average))
library(factoextra)
datos <- USArrests
datos <- scale(datos)
set.seed(101)
fviz_dend(x = hc_euclidea_complete, k = 4, cex = 0.6) +
geom_hline(yintercept = 5.5, linetype = "dashed") +
labs(title = "Herarchical clustering",
subtitle = "Distancia euclï¿½dea, Lincage complete, K=2")
fviz_cluster(object = list(data=datos, cluster=cutree(hc_euclidea_complete, k=4)),
ellipse.type = "convex", repel = TRUE, show.clust.cent = FALSE,
labelsize = 8)  +
labs(title = "Hierarchical clustering + Proyecciï¿½n PCA",
subtitle = "Distancia euclï¿½dea, Lincage complete, K=4") +
theme_bw() +
theme(legend.position = "bottom")
d <- dist(datos, method = "euclidean")
res.hc <- hclust(d, method = "ward.D2" )
# Pedimos que corte en 4 grupos
grp <- cutree(res.hc, k = 4)
# Visualizamos la particiï¿½n
plot(res.hc, cex = 0.6) ## agregar rectangulos
# Agrupamos en 4 clusters
res <- hcut(USArrests, k = 4, stand = TRUE)
# Visualizamos con color y rectangulo
fviz_dend(res, rect = TRUE, cex = 0.5,
k_colors = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07"))
fuzzy_cluster <- fanny(x = datos, diss = FALSE, k = 3, metric = "euclidean",
stand = FALSE)
head(fuzzy_cluster$membership)
fviz_cluster(object = fuzzy_cluster, repel = TRUE, ellipse.type = "norm",
pallete = "jco") + theme_bw() + labs(title = "Fuzzy Cluster plot")
fviz_nbclust(x = datos, FUNcluster = kmeans, method = "gap_stat", nboot = 500,
k.max = 15, verbose = FALSE, nstart = 50) +
labs(title = "Nï¿½mero ï¿½ptimo de clusters")
set.seed(896)
kmeans_gap <- clusGap(x = datos,
FUNcluster = kmeans,
K.max = 15,
B = 500,
verbose = FALSE,
nstart = 50)
kmeans_gap
kmeans_gap$Tab %>%
as.data.frame() %>%
rowid_to_column(var = "n_clusters") %>%
ggplot(aes(x = n_clusters, y = gap)) +
geom_line() +
geom_point() +
labs(title = "Clustering Kmeans") +
scale_x_continuous(breaks = 1:20) +
theme_bw()
library(ISLR)
data(NCI60)
str(NCI60)
View(data)
View(NCI60)
# Los datos estan almacenados en forma de lista, un elemento contiene los niveles
# de expresion y otro el tipo de cancer
expresion   <- NCI60$data
tipo_cancer <- NCI60$labs
head(expresion)
head(tipo_cancer)
# Una exploracion inicial de los datos permite conocer
#la cantidad de lineas celulares que hay de cada tipo
# de cancer
table(tipo_cancer)
expresion <- scale(expresion, center = TRUE, scale = TRUE)
matriz_distancias <- dist(x = expresion, method = "euclidean")
hc_completo <- hclust(d = matriz_distancias, method = "complete")
hc_average  <- hclust(d = matriz_distancias, method = "average")
hc_single   <- hclust(d = matriz_distancias, method = "single")
plot(hc_completo, labels = tipo_cancer, ylab = "", xlab = "", sub = "",
main = "Linkage completo", cex = 0.8)
abline(h = 139, col = "red")
plot(hc_average, labels = tipo_cancer, ylab = "", xlab = "", sub = "",
main = "Linkage average", cex = 0.8)
plot(hc_single, labels = tipo_cancer, ylab = "", xlab = "", sub = "",
main = "Linkage single", cex = 0.8)
abline(h = 139, col = "red")
clusters <- cutree(tree = hc_completo, k = 4)
table(clusters, tipo_cancer, dnn = list("clusters", "tipo de cancer"))
data("diabetes")
head(diabetes) # visualizamos las primeras filas
# Estandarizacion de variables
datos <- scale(diabetes[, -1])
head(datos)
# clustering basado en modelos
model_clustering <- Mclust(data = datos, G = 1:10)
summary(model_clustering)
# Grado de asignacion a cada cluster
head(model_clustering$z)
# Clasificacion final
head(model_clustering$classification)
# Curvas del valor BIC en funcion del numero de clusters para cada modelo.
# Atencion al orden en el que se muestra la variable horizontal, por defecto es
# alfabetico.
fviz_mclust(object = model_clustering, what = "BIC", pallete = "jco") +
scale_x_discrete(limits = c(1:10))
# Clusters
fviz_mclust(model_clustering, what = "classification", geom = "point",
pallete = "jco")
# Certeza de las clasificaciones. Cuanto mayor el tamaño del punto menor la
# seguridad de la asignacion
fviz_mclust(model_clustering, what = "uncertainty", pallete = "jco")
datos <- scale(iris[, -5])
km_clusters <- eclust(x = datos, FUNcluster = "kmeans", k = 3, seed = 123,
hc_metric = "euclidean", nstart = 50, graph = FALSE)
fviz_silhouette(sil.obj = km_clusters, print.summary = TRUE, palette = "jco",
ggtheme = theme_classic())
source("C:/Lab_Imp_1/labo/src/ranger/z433_ranger_BO.r", echo=TRUE)
source("C:/Lab_Imp_1/labo/src/lightgbm/z552_lightgbm_motivacional.r", echo=TRUE)
source("C:/Lab_Imp_1/labo/src/ranger/433_ranger_BO.r", echo=TRUE)
